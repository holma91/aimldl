# But what is a neural network?

(check notability for the math)

The last layer have 10 neurons, one for each num from 0 to 9. First layer has 784 neurons, because the images we are studying are 28 x 28 = 784px. The activation for a neuron in the first layer is based on how "lit up" the pixel it's representing is. Black is 0.00 and white is 1.00. The most "lit up" neuron in the last layer is the NNs choice.

A goal with a layered structure is that the middle layers kind of learns the different patterns and edges that classify a number. For example an 8 has 2 loops stacked on each other.

The activation of a neuron is basically:

- a measurement of how positive the relevant weighted sum is.

number of weights & biases = (784x16 + 16x16 + 16x10) + (16 + 16 + 10) = 13002

Learning is essentially: finding the right weights and biases

Nowadays ReLU is used instead of sigmoid. Mainly because it's easier to train.

# Gradient descent, how neural networks learn

The input to a cost function is the NNs weights and biases and the output is one number (the cost). The parameters are the training examples.

Is gradient descent about minimizing the cost? Maybe I should find the minimas of the cost function.

finding the local minimum: doable
finding the global minimum: crazy hard

The gradient of a function gives you the direction of steepest ascent.

What you need to to know at a high-level from multivariable calculus:
There exists a way to compute the vector that tells you what the downhill direction is and how steep it is.

The algorithm for computing the gradient efficiently is backpropagation.

When a NN is "learning", what it's doing is just minimizing a cost function.

Gradient descent is a way to converge to some local minima of a cost function.

The gradient vector encodes the relative importance of each weight and bias.

## Words

- Neuron (in our case):
  a thing that holds a number between 0 and 1.
  OR
  a function that takes in the outputs of all the neurons in the previous layer and spits out a number between 0 or 1.

- Activation:
  the value that the neuron holds.

- Weight:
  A value that every connection from layer x and layer x+1 has.

- Weighted sum:
  multiply every weight with it's activation in the previous layer. Sum all the products together.

- Sigmoid:
  Every negative input ends up close to 0. Every positive input ends up close to 1.

- the bias:
  What you put into the weighted sum before sigmoiding it. How high must the weighted sum be, before the neurons start getting meaningfully active?

- Cost function
  A way of telling how wrong the computer is, how much does it's result differ from the correct answer? Cost is small when the computer is correct and vice versa.

- Gradient descent
  A method for optimizing the cost function. Helps us find the global minima in the cost function.
