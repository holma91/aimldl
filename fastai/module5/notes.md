## General

Do not throw away data just because a column has a missing value, replace it with something reasonable instead.


### Tabular modeling
The objective is to predict the value in one column based on the values in the other columns.

Deep learning isn't everything. With tabular data, it's worth taking a look at classical ML algorithms.

Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:

- Ensembles of decision trees (i.e., random forests and gradient boosting machines), mainly for structured data (such as you might find in a database table at most companies)
- Multilayered neural networks learned with SGD (i.e., shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language)

While deep learning is nearly ALWAYS superior for unstructured data, the two methods give similar results on structured data. Ensembles of decision trees have the following pros:
- tend to train faster
- easier to interpret
- do not require special GPU hardware for inference at scale
- require less hyperparameter tuning


### Random forests

Bagging:
1. Randomly choose a subset of the rows of your data.
2. Train a model using this subset.
3. Save that model, and then return to step 1 a few times.
4. This will give you a number of trained models. To make a prediction, predict using all of the models, and then take the average of each prediction.

In essence a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters.

The OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row's error trees where that row was not included in training. Since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set (the rows that were not selected for that tree's training).


Random forests are easy to interpret, and as a result it's easy to remove unimportant stuff from it. 

Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable?

### Extrapolation 
important q: what algorithms generalizes the best to new data?

A tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time (e.g inflation) and you wish to make predictions for a future time.

Random forests basically can't extrapolate, so we need to make sure our validation set doesn't contain out-of-domain data.

### Neural Networks

## Words

- Continuous Variables
Numerical data such as age.

- Categorical Variables
Contains a number of discrete levels, such as gender.

- Embedding
A mapping of a categorical variable to a continuous vector. 

- Ordinal columns
Columns containing strings or similar, but where those strings have a natural ordering. E.g day of the week.