## General

**NLP**

What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before).

Self-supervised learning is often used for pretraining a model that will be used for transfer learning.

The steps for creating a language model:

1. Tokenization
2. Numericalization
3. Language model data loader creation
4. Language model creation


## Words

- Self-supervised learning
An approach in which a model is trained to solve a task using only input data and without the need for explicit labels. Basically, "training without external labels".

- Encoder
The model not including the task-specific final layer(s). Pretty much the same as "body" in the context of vision CNNs.





### Questions

RNNs vs Transformers?